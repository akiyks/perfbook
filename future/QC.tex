% future/QC.tex (7ab78678da33efd03a237070761df1ca888bee48)

\section{Quantum Computing}
\label{sec:future:Quantum Computing}

The ideas behind quantum computing (QC) go back
decades~\cite{RichardPFeynman1959RoomAtBottom,Bennett:1973:LRC:1664562.1664568,RichardFeynman1986QuantumMechanicalComputers},
but the technology to construct quantum computers has appeared
only recently~\cite{KamranKarimi2011D-WaveAdiabatic,IBM2016QuantumExperience}.
It has nevertheless generated considerable excitement,
even outside of the traditional technical
press~\cite{Economist2017QuantumComputingTechnologyQuarterly}.
This section gives a brief overview of the state of this
technology, based on use of IBM's Quantum Experience
website~\cite{IBM2016QuantumExperience} and on a superficial
literature review.
This overview indicates that although QC promises some
tantalizing possibilities, there are significant challenges
that it must overcome, including rapidly improving classic-computing
techniques and heuristics.

This section gives an overview of quantum computing from the perspective
of early 2017.
Note that this is concerned only with quantum computation.
Quantum communication and quantum cryptography are far more
advanced, are used in practice, and are beyond the scope of
this section.

Section~\ref{sec:future:Quantum Computing Players}
gives an overview of some of the more prominent companies working
with QC systems.
Section~\ref{sec:future:Quantum Computing Progress}
presents a brief evaluation of QC hardware trends to date.
Section~\ref{sec:future:Quantum Computing Challenges}
analyzes several challenges that QC must surmount in order to achieve
widespread use in practice.
Section~\ref{sec:future:Outlook} speculates on what must happen for QC
to ``take over the world''.
Finally,
Section~\ref{sec:future:QC Summary and Conclusions}
presents a summary and a few conclusions.

\subsection{Quantum Computing Players}
\label{sec:future:Quantum Computing Players}

This section provides an overview of several of the commercial
players in the QC arena.

D-Wave Systems~\cite{D-WaveSystemsHomePage}
released a quantum computing system
in 2011~\cite{WikipediaD-WaveSystems}, based on a d-wave
superconductor~\cite{MHSAmin2000D-Wave-superconductor}.
This system has been heavily discussed and
studied~\cite{KamranKarimi2011D-WaveAdiabatic}.
There has been some question as to whether D-Wave really exhibits quantum
properties~\cite{SeungWooShin2014IsDwaveQuantum}, but more recent
indications are that quantum effects really are
present~\cite{PhysRevA.91.042314,PhysRevX.4.021041}.
D-Wave has a number of customers and
collaborators~\cite{JeffreyBurt2014Google-QC-Chip,PatrickHarris2015QC-Google-NASA-DWave,ToddRWeiss2013Google-QC-AI-Lab}.
In addition, D-Wave is the undisputed champion in the area of commercially
available machines with large numbers of quantum bits, or
\emph{qubits}~\cite{WikipediaD-WaveSystems},
with a 2,048-qubit system delivered in
2017~\cite{AgamShah2016D-Wave-2000-qubit,BradJones2017D-Wave2000Sale}.
That said, these systems do not feature general-purpose qubits, but
rather qubits that are specialized to optimization problems.

In addition to its collaboration with D-Wave, Google has been working
with UCSB on QC hardware, including
QC memory~\cite{JaikumarVijayan2015Google-UCSB-QC-Memory}.

Intel is investing \$50M in quantum computing in partnership
with Google, NASA, and USRA~\cite{StaceyHigginbotham2015Intel-QC-invest-50M}.

Microsoft and UCSB have been collaborating on QC since
2004~\cite{PedroHernandez2014MicrosoftStationQ-QC}
and Microsoft has recently formed a new Quantum
division~\cite{PedroHernandez2016Microsoft-QC}.
Some regard Microsoft to be the leader in QC programming languages.
Microsoft believes that quantum chemistry will be the killer app
for QC~\cite{TomSimonite2017QC-MS-Chemistry}.

IBM has a long record of quantum work, including Bennett's
1973 work on the thermodynamic reversibility of QC
computation~\cite{Bennett:1973:LRC:1664562.1664568} and a 1980 issue
of IBM Journal of Research and Development focusing on Josephson
technology~\cite{1980:1663086}.
IBM continued this groundbreaking work with scanning tunneling
microscopy~\cite{Binnig1982SurfaceSTM}
and the atomic force microscope~\cite{1986PhRvL..56..930B},
which was famously used by Don Eigler to spell ``IBM'' on the atomic
scale~\cite{MalcolmWBrowne1990AFM-IBM}.

More recently, IBM has been focusing on quantum-computing hardware with
fewer qubits than that of D-Wave, but unlike D-Wave focuses on
general-purpose
qubits~\cite{BradJones2017IBM-QC-Announce,RobertHackett2017IBM-QC-Announce,AgamShah2017IBM-QC-50-qubit,DarioGill2017IBM-Universal-QC}.
In addition, IBM is the first to allow public access to its QC
hardware~\cite{IBM2016QuantumExperience,ArsTechnica2016IBMQuantumExperience,MikeVizard2017IBM-QC-Cloud},
which is based on transmon~\cite{WikipediaTransMon} work at
Yale University~\cite{PhysRevA.76.042319}.
This work has been extended
to increase coherence times into the tens of
microseconds~\cite{PhysRevLett.107.240501,PhysRevLett.111.080502,PhysRevB.86.100506}.
IBM's publicly available QC hardware was used by about 40,000 different
users running 275,000 experiments within its first
year~\cite{SeanMichaelKerner2017IBM-QC-API},
despite offering only five qubits having constrained entanglement.
As of May 2017, sixteen- and seventeen-qubit systems are available
from IBM, and in September Intel delivered a test prototype of
a seventeen-qubit system~\cite{Intel2017delivers17qubit}.

In short, QC is generating great excitement and receiving substantial
investment.
It is quite possible that a yet-as-unknown QC player will drive a
major game-changing breakthrough.
In the meantime, the next section evaluates technical progress in the
QC arena.

\subsection{Quantum Computing Progress}
\label{sec:future:Quantum Computing Progress}

QC systems have been making substantial Moore's-Law-style progress
in a number of technical areas.

\begin{table}
\renewcommand*{\arraystretch}{1.2}
\rowcolors{1}{}{lightgray}
\centering\small
\begin{tabular}{lrS[table-format = 4.0]S[table-format = 1.1]}
\toprule
System
	& Availability
		& \multicolumn{1}{r}{Qubits}
			& \multicolumn{1}{r}{\parbox[b]{.5in}{Years per\\Doubling}} \\
\midrule
D-Wave One
	& May 2011
		& 128
			& 1.4 \\
D-Wave Two
	& May 2013
		& 512
			& 1.9 \\
D-Wave 2X
	& Aug 2015
		& 1152
			& 1.7 \\
D-Wave 2000Q
	& Jan 2017
		& 2048
			& \multicolumn{1}{c}{$-$} \\
\bottomrule
\end{tabular}
\caption{D-Wave Qubit Growth Rate}
\label{tab:future:D-Wave Qubit Growth Rate}
\end{table}

For example, Table~\ref{tab:future:D-Wave Qubit Growth Rate} shows D-Wave's systems,
availability dates, numbers of qubits, and years per doubling
from that year to the present.
This data hints at a Moore's-Law-like growth in qubits, albeit from
an extremely limited data set.
Further discussion will use the (optimistic) long-term qubit
doubling duration of 1.4 years.
IBM Quantum Experience's May 2016 offering had but five qubits, but its
May 2017 offering has sixteen qubits.
This represents a qubit doubling duration of less than eight \emph{months},
but it remains to be seen whether IBM can sustain this pace.
% bc -l: 12*l(2)/l(16/5)
On the other hand, if QC can make good use of the process technology
developed for classic computing, there is some possibility that the number
of qubits might increase quite suddenly by many orders of magnitude.

Another approach is of course to reduce number of qubits required for a
given problem~\cite{SergeyBravyi2017-QC-SimulateFermionicHamiltonians}.
However, as will be seen in Section~\ref{sec:future:Error Rate}, the need for
quantum error correction increases the number of qubits required.

\begin{figure}[tb]
\centering
\resizebox{3in}{!}{\rotatebox{270}{\includegraphics{future/T2h1lc19xmqrdlsor}}}
\caption{Coherence Time Trend}
\label{fig:future:Coherence Time Trend}
\end{figure}

Another key element of QC progress is decoherence time.
QC decoherence results from the fact that qubits are fragile, and
decay over time.
Of course, this is not unprecedented: After all, classical computing's
ubiquitous dynamic RAM must be periodically refreshed.
However, until someone comes up with a practical way to refresh
qubits~\cite{GiorgioColangelo2017QC-SpinAngleAmplitude},
increasing decoherence time allows a quantum algorithm to do more
processing.
Coherence time for superconducting qubits seems to be doubling every year,
as shown in
Figure~\ref{fig:future:Coherence Time Trend}~\cite{IBM2016QuantumExperience}.
Extrapolating this trend suggests that 10-second coherence times might
be available in ten years time, which would bring more complex and
long-running QC algorithms into the realm of practicality, and might
also reduce the need for complex error-correction schemes.

Finally, increasing the number of qubits that can be
entangled (and the duration of the entanglement) is important for
quantum algorithms such as Shor's integer-factorization
algorithm~\cite{Shor:1997:PAP:264393.264406,Kendon:2006:ERS:2011698.2011704}.
Murphy's Law would suggest that increasing the number of qubits that
can be entangled would also decrease decoherence time, but time will tell.

There are some tantalizing hints that quantum-state readout might become
more accurate~\cite{GiorgioColangelo2017QC-SpinAngleAmplitude},
but it is not yet clear that these techniques apply to QC
(as opposed to sensing and spectroscopy).

Researchers recently achieved entanglement of 3,000
rubidium atoms in gaseous state at a few tens of
microkelvins~\cite{RobertMcConnell2015QC-Entangle3000Atoms},
though it is unclear how this could be adapted for use in
a QC system, which currently feature highly structured non-gaseous
collections of qubits.
Nevertheless, the current literature does not appear to provide enough
data to permit reasonable extrapolation of entanglement capabilities.\footnote{
	Which is quite possibly the fault of the editor rather than that
	of the literature.}
Trends regarding QC operation times seem to be similarly obscure.

Despite all this progress, QC face significant challenges, which are
the subject of the next section.

\subsection{Quantum Computing Challenges}
\label{sec:future:Quantum Computing Challenges}

Progress on QC has been exciting and good to see, but these systems are
still quite crude by the standards of conventional computing.
As Scott Crowder of IBM put it,
``It's the 1940s again''~\cite{BradJones2017IBM-QC-Crowder}.
For purposes of comparison, the oldest intact computer, the
University of Melbourne's 1949
CSIRAC~\cite{CSIRACMuseumVictoria,CSIRACUniversityMelbourne},
ran at a core clock frequency of 1\,kHz, consumed 30\,kW of power,
weighs three metric tons,
is constructed of 2,000 vacuum tubes, and has 768 words of RAM
implemented with acoustic mercury delay lines.
And these last are two reasons why it is ``intact'' rather than
``operational'', given that
neither metallic mercury nor exposed 600-volt wiring are
looked upon favorably in 2017.\footnote{
	Both were considered to be perfectly acceptable as late as the
	early 1960s.
	Which is OK.
	The denizens of the 2060s will be no doubt be equally horrified
	by any number of unremarkable 2017 practices.}
The steel tubes that once served as CSIRAC's main memory are therefore
empty of mercury and the wiring is therefore free of current.
Furthermore, since CSIRAC, tubes have been obsoleted by discrete
transistors which were in turn obsoleted by multiple generations of
integrated circuits.
Mercury delay lines were obsoleted by glass delay lines which were
obsoleted by magnetic core memory which were obsoleted by
semiconductor DRAM, which might be on its way to being obsoleted
by non-volatile RAM (NVRAM).

Nevertheless, the CSIRAC is believed to be the first computer to
play a game and to play music.
Similarly, we should expect future QC systems to look much different
than current prototypes, but we nevertheless have reason to hope that,
like CSIRAC, current QC prototypes will achieve notable milestones.

To shine some light on QC areas needing improvement,
Section~\ref{sec:future:Programming Model} looks at challenges posed by the QC
programming model (and attempts to demystify some aspects),
Section~\ref{sec:future:Error Rate} looks at challenges surrounding qubit
error rates,
Section~\ref{sec:future:Thermodynamics} presents energy-efficiency challenges
posed by the ever-inconvenient Laws of Thermodynamics,
Section~\ref{sec:future:Heuristics} gives an overview of competitive
challenges from the combination of classical computers and heuristics,
and
Section~\ref{sec:future:Mathematical Advances} hints at possible competitive
challenges from mathematicians.

\subsubsection{Programming Model}
\label{sec:future:Programming Model}

\begin{figure}[tb]
\centering
\resizebox{2.5in}{!}{\includegraphics{future/Bloch_Sphere}}
\caption{Qubit as Bloch Sphere}
\label{fig:future:Qubit as Bloch Sphere}
\end{figure}

The QC programming model is at best an acquired taste for developers
with classic-computing experience.
The remainder of this section covers qubits,
quantum entanglement,
and
the likely relationship between QC hardware and classic computer hardware.

\paragraph{Qubit}

A qubit is sort of like a classic-computing bit, but only sort of.
A qubit is said to:

\begin{enumerate}
\item	Be represented by a Bloch sphere, as shown in
	Figure~\ref{fig:future:Qubit as Bloch Sphere}.
\item	Collapse to a zero ($\ket{0}$) or a one ($\ket{1}$) if measured,
	with probability being a function of the relative distance from
	$\ket{0}$ and $\ket{1}$, but projected onto the Z\=/axis.
	Thus, a qubit on the equator of the Bloch sphere has a 50\,\%
	probability of being measured as a one or as a zero, while
	a qubit on the 45\textdegree-north latitude would have
	a 14\,\% chance of being measured as one and 86\,\% chance
	of being measured as zero.
	This situation naturally causes developers to prefer a line
	segment---or a classic-computing bit---over a sphere.
\item	Support only rotations on the Bloch sphere (in addition to
	the measurement operation).
\end{enumerate}

It appears that the need for a Bloch sphere is mainly dictated by \#3,
the limitations on the quantum-mechanical operations currently available
on the physical entities that represent qubits.

The basic non-entangling operators supported by IBM's Quantum Experience
are as follows:

\begin{description}
\item[\qop{H}\,:]
	Rotate 180\degree{} ($\uppi$ radians) about the Bloch-sphere
	X-Z axis, that is, about the 45\degree{} line on the
	X-Z plane.  This rotates $\ket{0}$ to the point at which the
	positive X\=/axis intersects the Bloch sphere, and rotates $\ket{1}$
	to the point at which the negative X\=/axis intersects the Bloch
	sphere.
	Either way, we get a qubit that is 50\,\% one and 50\,\% zero.
\item[\qop{S}\,:]
	Rotate 90\degree{} ($\frac{\uppi}{2}$ radians) about the
	Bloch-sphere Z\=/axis, which has no effect on qubits in the
	$\ket{0}$ or $\ket{1}$ states.
\item[\qop{S}$^{\bm{\dagger}}$:]
	Rotate $-90\degree$ ($-\frac{\uppi}{2}$ radians) about the
	Bloch-sphere Z\=/axis, which has no effect on qubits in the
	$\ket{0}$ or $\ket{1}$ states.
	This operator is the inverse of \qop{S}.
\item[\qop{T}\,:]
	Rotate 45\degree{} ($\frac{\uppi}{4}$ radians) about the
	Bloch-sphere Z\=/axis, which has no effect on qubits in the
	$\ket{0}$ or $\ket{1}$ states.
\item[\qop{T}$^{\bm{\dagger}}$:]
	Rotate $-45\degree$ ($-\frac{\uppi}{4}$ radians) about the
	Bloch-sphere Z\=/axis, which has no effect on qubits in the
	$\ket{0}$ or $\ket{1}$ states.
	This operator is the inverse of \qop{T}.
\item[\qop{X}\,:]
	Rotate 180\degree{} ($\uppi$ radians) about the Bloch-sphere
	X\=/axis, which takes $\ket{0}$ to $\ket{1}$ and vice versa.
\item[\qop{Y}\,:]
	Rotate 180\degree{} ($\uppi$ radians) about the Bloch-sphere
	Y\=/axis, which also takes $\ket{0}$ to $\ket{1}$ and vice versa.
\item[\qop{Z}\,:]
	Rotate 180\degree{} ($\uppi$ radians) about the Bloch-sphere
	Z\=/axis, which has no effect on qubits in the $\ket{0}$ or
	$\ket{1}$ states.
\end{description}

\begin{figure}[tb]
\centering
\resizebox{2.5in}{!}{\rotatebox{270}{\includegraphics{future/QC-FormConstant.pdf}}}
\caption{QC Program as Quantum Experience Score}
\label{fig:future:QC Program as Quantum Experience Score}
\end{figure}

Measurement causes the qubit to collapse to either one or zero, based
on the z-coordinate of the qubit.
The probability of collapse to zero is:

\begin{equation}
	\frac{1+z}{2}
\end{equation}

Similarly, the probability of collapse to one is:

\begin{equation}
	\frac{1-z}{2}
\end{equation}

Thus, one (limited) way to think of a qubit is as a fixed-point number
ranging between zero and one, inclusive, based on these probabilities
of collapse.
Constants may be formed by starting with (say) a $\ket{0}$ qubit and
applying sequences of \qop{H}, \qop{S}, and \qop{T} operations.
For example, the constant $0.14$ can be formed by applying an
\qop{H}, \qop{T}$^\dagger$, and another \qop{H}
operation on a $\ket{0}$ qubit as shown in
Figure~\ref{fig:future:QC Program as Quantum Experience Score},
in a manner not entirely unlike constant formation on classic
computers with small immediate fields.
However, given that IBM Q quantum operations consume on the
order of 50-60~nanoseconds, this series of operations would
consume around 150-180~nanoseconds.
To measure the result to two digits would require the operation to
be repeated on the order of 100 times, consuming 15-18~microseconds,
ignoring setup and measurement time.
During that time, classic-computing floating-point arithmetic could not
only invert a modest matrix, it could also do so with more than
ten digits of precision.

If this was all that QC provided, QC would be a rather slow and very
low-quality analog computer.
Given that digital computers obsoleted analog computers some decades back,
this analogy might lead many developers to ignore QC and to
use classic-computing floating point instead.
However, QC provides a powerful capability covered in the next section.

\paragraph{Entanglement}

QC has the \qop{CNOT} or \emph{controlled-NOT} operator that
\emph{entangles} the pair of qubits operated on.
Different uses of \qop{CNOT} can force the two qubits to have the same
value, opposite values, or other combinations of values (roughly speaking)
defined by a Bloch-sphere vector.
Entanglement can be used to implement constraints
on the relationships of the entangled variables to each other, which
could potentially make QC handle large optimization problems very
efficiently.

Multiple \qop{CNOT} operations can (in theory) entangle arbitrarily
large numbers of qubits, which could replace very large numbers of
classic-computing data structures representing relationships between
different entities with entanglement of the qubits
representing these entities.
In theory, this could greatly reduce computational complexity, with
Shor's polynomial-time algorithm for integer
factorization~\cite{Shor:1997:PAP:264393.264406}
being perhaps the best-known example.
In practice, this sort of computation will require substantial
improvements in QC hardware, though there is reason to hope
for such improvement~\cite{RobertMcConnell2015QC-Entangle3000Atoms}.
Many developers might also hope for advances in QC programming languages,
for example, language constructs that do more than a single quantum
operation on a single qubit, however, the current state of the
QC art dictates that quantum operations are precious and must
be carefully conserved.
On the other hand, those of us with ample grey hair might actually
feel a tinge of nostalgia for those long-lost years of our youth when
classic-computing instructions and memory had to be just as carefully
conserved.

\paragraph{QC as Computational Accelerator}

Given the small number of qubits and limited types of operations available
on those qubits, current QC hardware is clearly not going to run anything
resembling a modern operating system, let alone a modern software stack.
Instead, QC hardware would more likely be deployed as an accelerator,
similar to a GPGPU or FPGA,
interesting speculation about quantum operating systems
notwithstanding~\cite{HenryCorriganGibbs2017QCOS}.
This situation is similar to the now-ancient external interrupt
controllers, floating-point accelerators, and vector units that have
long since been pulled onto the CPU chip.
In contrast, as we will see in Section~\ref{sec:future:Thermodynamics}, 
it is very unlikely that QC hardware will be pulled into conventional
CPUs, at least not unless future QC hardware runs at much higher
temperatures.

In the near term, there are likely to be strict precision limitations
on many types of QC computations.
For many problems this is OK.
After all if the input data is only precise to two digits, it should
be OK for the output data to be precise to only two digits.
If additional precision is required, the QC system can do the
initial computation to two digits, and this result can be used
as the starting point for a classic computation.
This has proven a useful tactic for problems whose classic-computing
algorithms converge very slowly far from the solution, but very
quickly near the solution~\cite{JakubKurzak2007MixedPrecision}.
QC would be used to handle the portion of the computation for which
classic computing is too slow, and classic computing would be used
for the portion of the computation for which QC is too inaccurate.

It is hoped that splitting the problem over classic computing and
QC hardware will result in great speedups, but there can be no denying
that it greatly complicates the programming model.

Perhaps some time in the future it will be possible to extract
state, including superposition and entanglement, from a QC system,
and then reload it at a later time.
Until this is possible, QC systems are dedicated accelerators
that cannot be context-switched, except perhaps by partitioning
a large QC system into smaller pieces, each piece being used by
a different application.
Of course, these sorts of dedicated-hardware approaches are perfectly
acceptable in situations for which a batch (rather than timesliced)
computing model is suitable.

Developers wishing more information on QC should refer to QC textbooks
and IBM's Quantum Experience~\cite{IBM2016QuantumExperience}.
Additional comparisons between QC and classic computing may be
found in Sections~\ref{sec:future:Heuristics} and~\ref{sec:future:Outlook}.

\subsubsection{Error Rate}
\label{sec:future:Error Rate}

Quantum effects are subtle and subject to errors, as has been
well-known all the way back to
Heisenberg's uncertainty priniciple~\cite{WeinerHeisenberg1927Uncertain}
(or less well-known, but even more mind-bendingly,
Bell's theorem~\cite{JohnSBell1964EPRparadox}).
Error-correcting codes have therefore been
proposed~\cite{ADCorcoles2015QuantumErrorDetection}.
Other researchers are instead working to reduce QC error
rates~\cite{PhysRevB.77.180502,PhysRevLett.107.240501,PhysRevLett.111.080502,PhysRevB.86.100506,KristanTemme2016QC-error-mitigation}.
This work has resulted in decoherence times approaching 100~microseconds,
and decoherence times are increasing dramatically, as discussed in
Section~\ref{sec:future:Quantum Computing Progress}.
However, although a 100-microseconds decoherence time is impressive for
a QC system, it does not look at all good compared to the 64-millisecond
refresh times normally specified for DRAM.\footnote{
	And these refresh times are set conservatively.
	DRAMs can typically hold charge for 1-10~\emph{seconds}.}

Furthermore, given that IBM~Q quantum operations consume on the order of
50-60~nanoseconds, it is not possible to carry out very many
operations on a given qubit before it decoheres.
For example, an algorithm requiring 10,000 quantum operations on
a single qubit must wait for order-of-magnitude advances in
the QC state of the art.

This situation should motivate additional research into extending
coherence times, and in fact a 2013 paper demonstrated coherence
times of more than
39~\emph{minutes}~\cite{KamyarSaeedi2018QC-39-minutes}.
Unfortunately, the quantum states used by this work involve atomic nuclei,
which in turn require bulky nuclear magnetic resonance (NMR) machinery
to read and write state, and the reading and the writing takes place
at 4\,K, that is, at the temperature of liquid helium.
However, between reading and writing, the temperature of the sample may
be raised to room temperature for extended periods without affecting
the quantum state.

If the NMR requirement was not bad enough, many quantum algorithms require
entanglement~\cite{PeterWSchor2001QuantumAlgorithms}.
Unfortunately, NMR systems use strong magnetic fields to align
nuclear spins, and we cannot expect the exceedingly weak magnetic
field of one nucleus to interact strongly with an adjacent nucleus,
especially not with the electron clouds in the way.\footnote{
	The magnetic fields of nuclei \emph{can} interact directly
	via the nuclear Overhauser effect~\cite{PhysRev.92.411},
	but this effect follows not merely the inverse square law,
	but instead an inverse sixth power law!}
Now, it is possible to use higher-energy nuclear state transition,
whose photons are easily able to penetrate electron clouds.
Unfortunately, these photons are also called ``gamma rays'', which enjoy
the same sterling safety reputation enjoyed by the mercury-filled steel
tubes that served as CSIRAC's mercury-delay-line memory.
Worse yet, these gamma rays must be emitted in directions precisely
aligning with the crystal lattice of whatever material is subjected
to this punishment.
On the plus side, given some magical (and radiation-proof) way to absorb
momentum at opposite faces of this crystal, one might use these highly
directional gamma rays to construct the gamma-ray laser that appears in
so many science-fiction stories.
A gamma-ray laser is no doubt an spectacularly bad idea, but there is no
denying that it is also an spectacularly cool bad idea.\footnote{
	Hey, what would happen if someone took a sample of
	cobalt-60 and used an NMR to perfectly align all of its
	atomic nuclei!~\cite{1957PhRv..105.1413W}}

Make no mistake, this 39-minute coherence time is an impressive
achievement, but in the absence
of nanoscale NMR systems~\cite{HJMamin2013QC-nanoscale-NMR} and
highly directional and highly controlled gamma rays, it is
hard to imagine creating an efficient computer based on this approach.

The extreme impracticality notwithstanding, the relatively warm operating
temperatures are important, as discussed in
Section~\ref{sec:future:Thermodynamics}.

As long as use of nuclear quantum states for QC resides in the
realm of science fiction, we must use more fragile qubits
and error correction.
One approach is use of multiple physical qubits to represent
a single logical qubit, continuously refreshing state in an
manner reminiscent of DRAM~\cite{DanielThomasSankPhD}.
The more qubits, the lower the error rate must be, with error
rates on the order of $10^{-8}$ required for algorithms using
a few hundred qubits~\cite{DanielThomasSankPhD}.

\subsubsection{Thermodynamics}
\label{sec:future:Thermodynamics}

QC computation is thermodynamically reversible, generating
very little waste heat~\cite{Bennett:1973:LRC:1664562.1664568,RichardFeynman1986QuantumMechanicalComputers}.
This means that in theory, quantum computers can avoid the
Landauer limit~\cite{Landauer:1961:IHG:1661184.1661186}
of $kT \ln 2$, where $k$ is the Boltzmann constant and $T$ is the
temperature in degrees Kelvin.
Given that the Boltzmann constant is $1.38 \times 10^{-23}$\,J/K,
and given the 0.015\,K operating temperatures that IBM's Quantum Experience
hardware runs at, this limit is indeed low: $1.43 \times 10^{-25}$\,J.

However, because of its thermodynamic reversibiltiy,
QC is governed by an even lower limit:

\begin{equation}
	\upDelta E \geq \frac{\hbar}{2 \upDelta t}
\end{equation}

Here $\upDelta E$ is the energy required to change the qubit in Joules,
$\upDelta t$ is the time taken to change the qubit in seconds, and
$\hbar$ is Planck's constant, which is $6.62 \times 10^{-34}$\,J$\cdot$s.
For the 50-nanosecond switching times of IBM's Quantum Experience
hardware, this limit is $5.52 \times 10^{-27}$\,J, more than an order
of magnitude less than the Landauer limit.

Both of these limits are incredibly small, which holds out the promise
of insanely energy-efficient computation, except that
in practice, things don't work quite so nicely.
For example, additional waste heat will be generated
by initialization and measurement of the quantum state;
by ionizing radiation;
by thermal conduction, convection, and radiation from
the QC's room-temperature surroundings;
and
by the need for quantum error correction, whether that error correction
is implemented by duplicate qubits or by duplicate runs of the QC
program.
Unfortunately, it is not just the amount of heat generated that is
important, but also the temperature at which this heat is generated.

\newcommand{\TLo}{T_\mathrm{L}}
\newcommand{\THi}{T_\mathrm{H}}
\newcommand{\CPf}{C_\mathrm{P}}

The thermodynamic theoretical limit on the ability of a refrigerator
to transport heat from a low temperature ($\TLo$) to a high temperature
($\THi$) is given by the coefficient of performance ($\CPf$):

\begin{equation}
	\CPf = \frac{\TLo}{\THi - \TLo}
\end{equation}

\begin{table}
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.25}
\centering\footnotesize
\begin{tabular}{p{1.7in}p{0.95in}}
\toprule
Law of Thermodynamics
	& English Translation \\
\midrule
Energy is conserved.
	& Can't win! \\
Entropy increases in closed systems.
	& Can't break even! \\
Entropy approaches a constant value as temperature approaches absolute zero.
	& Can't leave the game! \\
\bottomrule
\end{tabular}
\caption{The Three Laws of Thermodynamics}
\label{tab:future:The Three Laws of Thermodynamics}
\end{table}

This equation is related to the ever-inconvenient Laws of Thermodynamics,
fancifully illustrated in
Table~\ref{tab:future:The Three Laws of Thermodynamics}.

The nominal temperature for IBM~Q is 15~millikelvins, which certainly
qualifies as a low $\TLo$.
Let's assume $\THi$ is 293\,K (room temperature),
in which case $\CPf$ is $0.000051$.
This in turn means that it requires \emph{at least} one watt of
power into the refrigeration unit to transport $0.000051$~watts
of waste heat from the 15~millikelvin IBM~Q out to room temperature.
Put another way, 19.5\,kW is required to remove one watt of waste heat.
Thus, ``very little waste heat'' can nevertheless generate a significant
power bill for refrigeration, albeit less than two-thirds of the power
consumption of the CSIRAC machine discussed in
Section~\ref{sec:future:Quantum Computing Challenges}.
In addition, efficiently transporting data across such a large
temperature differential can be challenging.
These are but two reasons why you should not expect an IBM~Q in your
smartphone just yet.
This situtation will also prevent current QC hardware from being pulled
onto the CPU chip---the refrigeration costs would be prohibitive for
a full-power chip, even allowing for the lower-power circuitry possible
at low temperatures.\footnote{
	Interestingly enough, ARM's low-power CPU family would face
	a less daunting refrigeration barrier to QC-on-a-chip.}

\begin{table}
\rowcolors{1}{}{lightgray}
\renewcommand*{\arraystretch}{1.2}
\centering\small
\begin{tabular}{lS[table-format = 3.3]S[table-format = 1.6]S[table-format = 5.1]}
\toprule
Situation
	& \multicolumn{1}{c}{$T$ (K)}
		& \multicolumn{1}{c}{$\CPf$}
			& \multicolumn{1}{r}{\parbox[b]{.75in}{Power per watt\\waste heat (W)}} \\
\midrule
Dry Ice
	& 195
		& 1.990
			& 0.5 \\
Liquid N$_2$
	& 77
		& 0.356
			& 2.8 \\
Liquid H$_2$
	& 20
		& 0.073
			& 13.7 \\
Liquid He
	& 4
		& 0.0138
			& 72.3 \\
IBM~Q	& 0.015
		& 0.000051
			& 19 500.0 \\
\bottomrule
\end{tabular}
\caption{Refrigeration Power Consumption}
\label{tab:future:Refrigeration Power Consumption}
\end{table}

One way to reduce the barrier to QC-on-a-chip would be to raise
QC's operating temperature.
Higher temperatures help because the refrigeration power required
decreases dramatically with increasing temperature, as shown in
Table~\ref{tab:future:Refrigeration Power Consumption}.
Therefore, high-temperature quantum systems amenable for QC use would
greatly improve energy efficiency and ease transfer of data to and from
the QC.
Unfortunately, given the current state of the QC art, higher
temperatures also sharply decrease coherence times.
Therefore, for the foreseeable future, QC systems will need
energy-hungry refrigeration systems, which means that QC
systems need a high-value killer app.

Of course, if the value of the killer app is sufficiently high,
19.5\,kW might be considered cheap.
In this case, in the spirit of
``plenty of room at the bottom''~\cite{RichardPFeynman1959RoomAtBottom},
we might want even lower temperatures.
For example, Bose-Einstein condensates
(BECs)~\cite{NIST2001BoseEinsteinCondensate}
form in the sub-microkelvin range, exhibiting interesting
macro-scale quantum effects.
It is not clear how one would construct any sort of computer from
these condensates, nor how one would go about providing the 1.6\,GW
required to remove one watt of waste heat from a BEC---after all,
even Emmett Brown's fictional flux capacitor required only 1.21 gigawatts.
However, much remains to be explored in this realm
of low-temperature exotic states of matter and energy, to say
nothing of new materials, for but one example,
perovskite~\cite{ZhengChen2016PerovskiteQDMOFthinFilm}.
Other avenues include increased pressure, given that diamond anvil
cells~\cite{Weir1959DiamondAnvilCell} can now reach
640\,GPa~\cite{LeonidDubrovinsky2012640GPaDiamondAnvilCell},
which is almost double the estimated pressure at the center of the earth.
Such exploration is of course pure research, but if QC is at 1940s levels
of development, pure research should have a significant role to play.

Either way, a killer app is absolutely necessary.
Optimization might well be one such killer app, but as we will see in the
next section, it has serious competition.

\subsubsection{Heuristics}
\label{sec:future:Heuristics}

Moore's Law for SAT~\cite[Fig.~2.3]{Kroening:2008:DPA:1391237} shows
that industrial-strength SAT solvers have advanced from handling
``in a few hours'' about
100 variables in 1990 to handling about 1,000,000 in 2010, an increase
of four orders of magnitude in 20 years.
This represents a doubling every 1.5 years, and this progress has
continued.
For example, in 2016, a software-verification application solved a
90,000,000-variable problem in 84 hours~\cite{LihaoLiang2016VerifyTreeRCU}.
Others have noted similar exponential
progress~\cite{SharadMalik2010SATSolverHistory,SATCompetition2002,vanHarmelen:2007:HKR:1557461,Malik:2009:BST:1536616.1536637,JamesEzick2014ExtremeSAT}.
This progress has been due to great advances in SAT-solver
heuristics~\cite{Kroening:2008:DPA:1391237,Zhang:2002:QEB:647771.734434,SharadMalik2010SATSolverHistory,Malik:2009:BST:1536616.1536637,Audemard:2009:PLC:1661445.1661509}.

Although there are special SAT cases that have not yet succumbed to
heuristics, perhaps most famously those requiring application of the
pigeonhole principle~\cite[page~38]{Kroening:2008:DPA:1391237},
it seems safe to assume continued progress.
And similar progress has also been achieved for other hard
problems~\cite{WikipediaPrimalityTest,WikipediaTSP,WikipediaIntegerFactorization}.
This should not be surprising, given that there is great economic value
in improving logistics, hardware/software verification, electronic layout,
and other problems that reduce to SAT or to other famous hard problems.
This value can be expected to continue to drive significant research
and development in this area, perhaps incorporating additional
machine-learning work~\cite{ShaiHaim2009SAT-MachineLearning}.
For example, perhaps machine-learning techniques will allow difficult
cases to be detected so that alternative solution methods can be applied
as needed.

Of course, much of this work has been heuristic and/or probabilistic,
so it might at first glance seem unfair to compare them to
QC algorithms.
However, QC algorithms are inherently probabilistic due to error rates
and measurement uncertainties, so the comparison is in fact eminently
fair.
This could be a severe challenge to mainstream QC, given the potential
need for 90,000,000 qubits to address problems recently solved by
very ordinary classical single-threaded SAT
solvers~\cite{LihaoLiang2016VerifyTreeRCU}.

Nor are these considerations solely theoretical.
There has been work documenting a four-orders-of-magnitude
performance advantage of D-Wave machines compared to classical
software for certain algorithms~\cite{McGeoch:2013:EEA:2482767.2482797}.
This work achieved considerable traction in the popular
press~\cite{CharlesChoi2013D-WaveGoogleNASA}.
However, another researcher has shown that classical software running
single-threaded on desktop-class systems can
have four-order-of-magnitude performance advantages over
D-Wave hardware~\cite{AlexSelby2014D-Wave-vs-classical,AlexSelby2013D-WaveHarderQUBO}.
Still other researchers found no evidence of quantum speedup
from D-Wave hardware for selected problems, though there might well be
other problems for which D-Wave hardware produces such
speedups~\cite{AdrianCho2014QC-D-WaveNoSpeedup,TroelsFRonnow2014QC-D-WaveNoSpeedup}.
Perhaps surprisingly, a D-Wave researcher argues that the search for
quantum speedup is in some sense
problematic~\cite{MohammadHAmin2015QC-D-Wave-QuantumSpeedupProblematic},
however, it really does appear that evaluating QC system performance is
not at all
trivial~\cite{PhysRevLett.118.100601,ArsTechnica2017QC-SpeedTradeoffs}.

Nevertheless, limitations in hard-problem heuristics such as the
pigeonhole principle for SAT
solvers~\cite[page~38]{Kroening:2008:DPA:1391237}
provides some hope that a high-value QC killer app might someday emerge.

Unfortunately, heuristics are not the only threat to the utility
of QC systems.
Advances in mathematics might also be a potent threat, as we will see
in the next section.

\subsubsection{Mathematical Advances}
\label{sec:future:Mathematical Advances}

One of the more tantalizing promises of QC is Shor's
polynomial-time integer factorization
algorithm~\cite{Shor:1997:PAP:264393.264406,WikipediaShorsAlgorithm}.
However, current polynomial-factoring algorithms are not
inconsequential.
For example, the \co{maxima} program can factor a 59-digit number\footnote{
	\scriptsize
	63698321299468802831035558537099113360211126822411635339497,
	which is the product of 15780285428767, 15780285428771, and
	255798667878176814448339699861021.}
in less than 20 seconds single-threaded on an x86 laptop.
Of course, this is a very small number compared to the 1,000-digit
integers that would need to be factored in order to break RSA,
and if integer factorization is exponential, RSA is quite safe.

However, an unexpected classic-computer
polynomial-time integer primality test was recently
devised~\cite{ManindraAgrawal2004PrimesIsInP,WikipediaAKSPrimalityTest}.
Who is to say that a classic-computer polynomial-time integer
factorization algorithm won't soon follow?

It is all too easy to dismiss the possibility of mathematical solutions
to hard problems.
One way to avoid succumbing to this temptation is to consider the
long list of major advances over the last 50 years:

\begin{description}
\item[1970:] Proof that Hilbert's 10\textsuperscript{th} problem
	is unsolvable.
\item[1975:] Fractals.
\item[1976:] Proof of the four-color problem.
\item[1984:] First polynomial-time algorithm for solving linear
	programming problems.
\item[1994:] Proof of Fermat's Last Theorem.
\item[1998:] Proof of Kepler's conjecture.
\item[2002:] Polynomial-time integer primality test.
\item[2002:] Proof of Catalan's conjecture.
\item[2003:] Proof of the Poincar\'e conjecture.
\item[2004:] Proof of the classification of finite simple groups.
\item[2013:] Proof that there is no bound on the values of
	pairs of primes differing by a finite number.
\item[2015:] Quasi-polynomial time solution to the graph isomorphism
	problem.
\end{description}

Several of these problems had stood for centuries, perhaps most famously
Fermat's Last Theorem.
The 2013 prime-pair result represents the first significant step forward
on Euclid's 300BC twin-prime hypothesis, that is to say, the first
significant step forward in more than two millennia.

Perhaps there is a race to solve hard problems in mathematics on
the one hand and to perfect large-scale quantum computing on
the other.
If so, we can hope that the competition between these two approaches
will be good for all concerned.
The next section assesses QC's outlook based on its current
competitive position with respect to classical computing.

\subsection{Outlook}
\label{sec:future:Outlook}

So what is required for QC to take over the world?

This section looks at two aspects of this question:
(1)~``What is needed to make a production-quality QC system?'' and
(2)~``What is the QC killer app?''

\subsubsection{Production-Quality QC Systems}
\label{sec:future:Production-Quality QC Systems}

An industrial-strength QC system requires per-qubit error rates below the
$10^{-8}$ threshold, preferably far below.
Shor's algorithm requires that a great many of these qubits be entangled,
and also requires several thousand general-purpose qubits.
Beating classical SAT solvers will require many millions of qubits.
It must be possible to load data into and extract results from QC
systems with high bandwidth and low latency.
The power consumption of a QC system (including refrigeration) must be
commensurate with the value of the problem being solved, and must also
be competitive with classical computing systems solving the same problem.

At some point, an agreed-upon measure of the overall capability of a
QC system will be needed.
One early candidate is the concept of
\emph{quantum volume}~\cite{LevSBishop2017QuantumVolume}, which
incorporates number of qubits, number of operations until decoherence,
connectivity, and parallelism.
Past experience with other technologies indicates that quite a few more
measures will be proposed, with some being more self-serving than others.
Should multiple QC killer apps appear, it is likely that specialized
measures will be tuned to the requirements of a given app.

Of course, such specialization requires that there actually be killer
apps, which leads to the next section.

\subsubsection{QC Killer Apps}
\label{sec:future:QC Killer Apps}

This section evaluates five potential QC killer apps,
Simon's periodicity problem,
Shor's integer factorization algorithm,
Grover's search algorithm,
quantum mechanical dynamics,
optimization problems, and
gaming.

\paragraph{Simon's Periodicity Problem}
\label{sec:future:Simon's Periodicity Problem}

Simon's problem, as described by
Shor~\cite{PeterWSchor2001QuantumAlgorithms},
requires computing the periodicity of a function.
Let's assume that there is no polynomial-time classical
algorithm solving this problem.
The question then becomes ``Why is solving this problem valuable?''
At present, there is no known valuable use for this algorithm,
other than Shor's algorithm, which is considered separately.

\paragraph{Shor's Integer Factorization Algorithm}
\label{sec:future:Shor's Integer Factorization Algorithm}

Shor's algorithm factors integers in polynomial time.
This result would be quite valuable (if rather destructive),
but current QC systems are nowhere near able to run
Shor's algorithm on the thousand-digit numbers required
to break RSA cryptography.
This algorithm requires general-purpose highly entangled qubits,
so we must start with IBM~Q's May 2017 sixteen-qubit system.
Assuming D-Wave's 1.4~years per doubling, it will be about thirteen
years before QC systems can break current-day RSA.
% 1000 digits is about 1000*l(10)/l(2)=3322 bits
% Assume 1 qubit per bit for Shor's algorithm.
% At D-Wave doubling rate: 1.4*l(3322/16)/l(2)=13.1 years
On the other hand, if IBM sustains its 8-month doubling time, RSA has
less than five years to live.
% IBM from 5 to 16 qubits in a year: l(2)/l(16/5)=.59592202035757028109 years
% This is 12*l(2)/l(16/5)=7.15 months.
% So at IBM doubling rate: 0.5959*l(3322/16)/l(2)=4.6 years.
Of course, both cases assume that QC surmounts the challenges called
out in Section~\ref{sec:future:Quantum Computing Challenges}.
Furthermore, it seems likely that a real implementation of Shor's
algorithms would need additional qubits, as in about one hundred
million of them~\cite{RachelCourtland2017GoogleQC},
which might significantly extend RSA's lifespan.

Nevertheless, even those who are only slightly paranoid might consider
it to be not too early to start thinking in terms of replacing RSA.

\paragraph{Grover's Search Algorithm}
\label{sec:future:Grover's Search Algorithm}

Grover's algorithm searches an unordered list of $N$ items
in $\O{\sqrt N}$ time.
This is mainly intended for implicit search for solutions as opposed
to searching through data.
To see why, keep in mind that before any data can be searched,
that data list must be downloaded into the QC system, and that
this download will have computational complexity $\O{n}$, where
$n$ is the number of data items.
The competing classical system can use this time to sort the data
or to construct any desired index over the data, and the computational
complexity of these operations can be considered to be $\O{n \log_2 n}$,
after which the classical
system can carry out the search in $\O{\log N}$ time, which
is much faster than the $\O{\sqrt N}$ time promised by
Grover's algorithm.

\QuickQuiz{}
	What do you mean $\O{n}$ for classic-computing sorting/indexing
	and $\O{n \log_2 n}$ for classic-computing search?
	Hash tables do $\O{n}$ and $\O{1}$ respectively!!!
\QuickQuizAnswer{
	Fixed-size hash table lookups are $\O{n}$, not $\O{1}$.
	And for a resizing hash table, fairness dictates that the overhead
	of resizing be properly accounted for.

	That said, the example of a properly tuned fixed-size hash table
	is entirely appropriate for specific situations, and clearly
	illustrates why this section's asymptotic analysis is not at
	all unfair to quantum computing.
} \QuickQuizEnd

If there are to be $m$ searches, then the overall computational
complexity of Grover's algorithm is given by:

\begin{equation}
	n + m \sqrt n
\end{equation}

Similarly, for classical computing:

\begin{equation}
	\left( n + m \right) \log_2 n
\end{equation}

Forming the ratio, so that larger numbers indicate a win for classical
computing:

\begin{equation}
	\frac{n + m \sqrt n}{\left( n + m \right) \log_2 n}
\label{eq:Grover to Classic Overhead Ratio}
\end{equation}

Of course, one can pick $n$ and $m$ to favor either approach.
It makes little sense to choose small $m$ because the winner of that
race is a simple $\O{n}$ sequential scan.
More interesting scenarios use larger values of $m$.

The first scenario looks at
Equation~\ref{eq:Grover to Classic Overhead Ratio}
for large $n$ and $m$.
Let's keep life simple by first taking the limit as $m$ increases:

\begin{equation}
	\lim_{m\to\infty} \frac{n + m \sqrt n}{\left( n + m \right) \log_2 n}
	\Rightarrow \frac{\sqrt n}{\log_2 n}
\label{eq:sqrt n by log 2 n}
\end{equation}

Next, we take the limit as $n$ increases:

\begin{equation}
	\lim_{n\to\infty} \frac{\sqrt n}{\log_2 n}
	\Rightarrow \frac{\log 2}{2} \sqrt n
\end{equation}

This increases without limit, indicating the very large numbers of searches
over very large data sets favors classical computing.

The second scenario assumes that the dataset is large, but that only
some fraction $p$ of the dataset is searched for.
We therefore substitute $m = pn$ into
Equation~\ref{eq:Grover to Classic Overhead Ratio}:

\begin{equation}
	\frac{n + p n \sqrt n}{\left( 1 + p \right) n \log_2 n}
\end{equation}

Canceling $n$:

\begin{equation}
	\frac{1 + p \sqrt n}{\left( 1 + p \right) \log_2 n}
\end{equation}

Because we are interested in asymptotic behavior, we can ignore the
$1$ in the numerator, then cancel $p$, resulting in:

\begin{equation}
	\frac{\sqrt n}{\log_2 n}
\end{equation}

This is the same as Equation~\ref{eq:sqrt n by log 2 n},
so this arrangement also favors classical computing.

In a final attempt to show Grover's algorithm in a good light,
the third scenario assumes that the dataset is large, but that the portion
searched is given by $m = n^\alpha$.
Substituting into
Equation~\ref{eq:Grover to Classic Overhead Ratio}:

\begin{equation}
	\frac{n + n^\alpha \sqrt n}{\left( n + n^\alpha \right) \log_2 n}
\end{equation}

Simplifying:

\begin{equation}
	\frac{n + n^{\alpha + \frac{1}{2}}}{\left( n + n^\alpha \right) \log_2 n}
\end{equation}

The asymptotic behavior of the numerator and denominator depend on
the value of $\alpha$.
For the numerator (Grover overhead):

\begin{eqnarray}
	\alpha \leq \frac{1}{2} & \Rightarrow & n \\
	\alpha > \frac{1}{2} & \Rightarrow & n^{\alpha + \frac{1}{2}}
\end{eqnarray}

For the denominator (classical-computing overhead):

\begin{eqnarray}
	\alpha \leq 1 & \Rightarrow & n \log_2 n \\
	\alpha > 1 & \Rightarrow & n^\alpha \log_2 n
\end{eqnarray}

The first range is $\alpha \leq \frac{1}{2}$, where the ratio is as follows:

\begin{equation}
	\frac{n}{n \log_2 n} = \frac{1}{\log_2 n}
\end{equation}

And we finally have a regime that favors Grover's algorithm!
The reason is that the slowly growing value of $m = p^\alpha$ does
not give classical computing enough searches to make up for its
greater initialization-time computational complexity.

The second range is $\frac{1}{2} < \alpha \leq 1$,
where the ratio is as follows:

\begin{equation}
	\frac{n^{\alpha + \frac{1}{2}}}{n \log_2 n} =
	\frac{n^{\alpha - \frac{1}{2}}}{\log_2 n}
\end{equation}

Because we know $\alpha > \frac{1}{2}$ and of course $n \geq 1$,
we know that this result is bounded below by
Equation~\ref{eq:sqrt n by log 2 n}, so that this range again
favors classical computing.
The reason is that there are enough searches to make up for
classical computing's greater initialization-time computational
complexity.

The final range is $\alpha > 1$, where the ratio is as follows:

\begin{equation}
	\frac{n^{\alpha + \frac{1}{2}}}{n^\alpha \log_2 n}
\end{equation}

Cancelling $n^\alpha$ yields:

\begin{equation}
	\frac{\sqrt n}{\log_2 n}
\end{equation}

This is exactly
Equation~\ref{eq:sqrt n by log 2 n}, so that this range again
favors classical computing, though interestingly enough not by
as much as the 
$\frac{1}{2} < \alpha \leq 1$ range.

As long as there are enough searches to overcome classical computing's
greater initialization-time computational complexity, classical computing
beats Grover's algorithm.

That said, this analysis has some limitations:

\begin{enumerate}
\item	Explicit lists are assumed.
	Implicit lists might well favor quantum computing.
\item	Traditional sorting and indexing is assumed to result in
	the traditional $\O{\log N}$ computational complexity for
	classic-computing search.
\item	Quantum computing is assumed to be capable of handling
	very large data sets.
\item	Any required quantum error correction is assumed to incur
	constant overhead.
\item	The data is stored on classical-computing systems.
	The analysis changes dramatically if the full dataset
	can be stored in the quantum computer.
\end{enumerate}

Nevertheless, it does produce the useful rule of thumb that Grover's
algorithm can asymptotically prevail if, for a dataset of $n$ items,
$\sqrt n$ or fewer of them will be searched.

Again, most believe that Grover's algorithm would be better applied to
more general searches, including searching for solutions to optimization
problems.
Optimization via QC is taken up later in this section.

\paragraph{Quantum Mechanical Dynamics}
\label{sec:future:Quantum Mechanical Dynamics}

Simulating quantum mechanical dynamics.
It is believed that QC systems will be able to simulate themselves
exponentially faster than classical systems will be able to
simulate QC systems~\cite{Feynman1982}.
However, the geometric constraints inherent in manufacturable
QC systems suggests that classical systems have a chance
to efficiently simulate quantum systems with less
manufacturing-friendly geometries.
Microsoft is nevertheless betting on quantum chemistry as being
the initial QC killer app~\cite{TomSimonite2017QC-MS-Chemistry},
perhaps based on reconfigurable QC hardware.
Nor is Microsoft alone: Chinese researchers are working to apply
QC to model quantum photon interactions~\cite{StephenChen2017ChinaQC}.
In addition, IBM calls out chemistry as a possible QC application,
and expects to deliver a convincing proof of concept using a
50-qubit system.

In any case, given that quantum chemistry requires solving
Schr\"{o}dinger's equation, which in turn can require inverting
gigabyte-sized sparse matrices, progress in this area would
be quite valuable.
One of QC's competitors might be machine learning, given the positive
experiences with \url{https://fold.it}.
It is also possible that another competitor might turn out to be the
molecules themselves~\cite{Chin-wenChou2017ManipulateMolecule}, though
currently these methods are restricted to very small molecules.
But it is worth noting that anything using the molecules themselves
is by definition physical chemistry, and the big reason for applying
classical computing to the quantum mechanical dynamics was to reduce
cost and obtain results more quickly.

Of course, innovative combinations of physical chemistry and
classical computing might well produce even better results,
although a key word here is ``might''.
For example, one recent advance employs a non-crystalline glass phase
of water, extremely low temperatures, and an improved scanning
tunneling electron microscope to create atomic-scale images of
large proteins.
This advance garnered its inventors the 2017 Nobel Prize in
Chemistry~\cite{NobelPrizeChemistry2017,JohnTimmer2017ChemistryNobel},
and for good reason.
Nevertheless, this technique is specialized to certain types of molecules
in extreme conditions.

There are other problems that are provably out of the reach of
classical computing, such as simulating quantum thermal Hall conductance.
Unfortunately, such problems are also provably out of the reach of
quantum computing~\cite{ZoharRingel2017QuantizedGravityHall,RichardChirwin2017ThermalHallConductance}.

Thus, quantum chemistry might still be QC's best killer-app bet.

\paragraph{Optimization Problems}
\label{sec:future:Optimization Problems}

Earlier sections have discussed D-Wave's target market of optimization
problems.
Assuming that a QC algorithm for solving SAT requires only one qubit
per variable, and assuming that D-Wave continues doubling qubits every
1.4 years, it will be 35 years before a D-Wave machine can reproduce
Liang's SAT-based Linux-kernel RCU correctness
proof~\cite{LihaoLiang2016VerifyTreeRCU}.

Improved solutions to the famous Traveling Salesman Problem would
be extremely valuable in reducing costs (and environmental impacts)
of logistics, but current classic heuristics can find near-optimal
solutions for hundreds of cities~\cite{Martin:1992:LMC:2307953.2308141}
and polynomial-time algorithms that are guaranteed to find routes
that are no more than 40\,\% longer than optimal for arbitrarily
large numbers of cities~\cite{Sebo:2014:STN:2688265.2688281},
improving on the 50\,\% bound located a few decades
earlier~\cite{NicosChristofides1976TSP-FiftyPercent}.
As of 2006 TSP solvers were finding optimal solutions to
85,900-city problems~\cite{DLApplegate2007TSPtextbook}.
Assuming one qubit per city and assuming that D-Wave continues doubling
its systems' qubit counts every 1.4 years, it will take D-Wave more than seven
years to produce a system capable of handling this problem.
However, it seems likely that one qubit per road segment (rather than per city)
will be required, in which case far more qubits will be required,
resulting in a very long wait indeed.
Quantum computing might one day win this race, but there can be no doubt that
classic-computing heuristics are putting up an impressive fight.

\paragraph{Gaming}
\label{sec:future:Gaming}

No section on killer apps for any sort of computer system could possibly
be complete without some discussion of gaming.
And it turns out that the first QC computer games appeared in early
2017~\cite{JamesWootton2017IBMQEgame,JamesWootton2017IBMQEbattleship}.
These games are extremely primitive, but the real surprise is that
they exist at all.
That said, gaming and entertainment have been driving forces
behind a great many improvements in classical computing, so it is
only reasonable to assume that they will also have a role to play
in QC systems.

\paragraph{QC Killer App: Conclusions}
\label{sec:future:QC Killer App: Conclusions}

The two areas that seem to have the best chance of producing the killer
app that QC so badly needs are quantum mechanical dynamics (perhaps
most notably quantum chemistry) and optimization.
However, neither of these areas is the sole property of QC: Classical
computing is still very much in play.
In short, the jury is still out on QC, consistent with the
statement that ``this is the '40s''.

\subsection{QC Summary and Conclusions}
\label{sec:future:QC Summary and Conclusions}

Within the past decade, QC has made the move from pure theory to
commercially available systems along with publicly accessible hardware.
This is impressive progress, but the QC killer app has not yet been
identified.
One possible starting niche for QC contains problems that classic algorithms
have special difficulty with, for example, the SAT pigeonhole problem
or quantum chemistry.
That said, classic-computing algorithms and heuristics are improving
rapidly, which means that QC must hit a moving target.
The jury is therefore still out on the economic viability of QC systems,
but it seems likely that the competition between
QC and classic systems will benefit everyone.

Assuming that the QC community reacts well to this competition,
we can expect future QC systems to be as different from
today's prototypes as current classic systems are from the 1949 CSIRAC.
However, for the foreseeable future, QC systems are likely to be
architected as computational accelerators attached to classic computing
systems,
which means that large software constructs like operating systems will
continue to run on classic computing systems rather than on QC systems.
Current QC state-saving limitations suggest that QC hardware will be shared by
partitioning rather than by context switching, however, as is also
the case for a number of present-day devices that have large contexts.

Organizations and developers that can afford to do so should therefore
invest in both QC and classic systems.
However, those operating under tight constraints might reasonably choose
to continue focusing solely on classic computing systems, looking to
quantum effects only for quantum encryption/communication, which is
already used in practice.
